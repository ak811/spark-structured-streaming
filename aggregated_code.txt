PROJECT FILE TREE
================================================================================
spark-structured-streaming/
    task1.py
    data_generator.py
    task3.py
    task2.py
    README.md
    outputs/
        task_3_samples/
            part-00000-ac754502-3e6e-41cb-bc0a-fec573e1f6c9-c000.csv
            part-00000-41e3f20d-35a3-4351-b1eb-29cedf8cb6e1-c000.csv
            part-00000-cb047fb3-3acd-45f7-a900-cc4a4605ce62-c000.csv
        task_2_samples/
            part-00000-33b3c6ca-6402-43c7-a6f9-3230cbb92e74-c000.csv
            part-00000-27fe34ce-6212-4f58-b76e-e4ada7ecba20-c000.csv
            part-00000-0a853991-4261-4cc5-a38f-0842b594b58c-c000.csv
        task_1_samples/
            part-00000-26dbff93-1398-4ceb-8476-42791e981115-c000.csv
            part-00000-c37c7140-3a75-4387-90e8-6c4e43ffa00c-c000.csv
            part-00000-db00ef55-2a04-4b10-bb9f-08313ba7aee6-c000.csv


/home/data/akhalegh/utils/spark-structured-streaming/task1.py
================================================================================
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# Spark session
spark = (
    SparkSession.builder
    .appName("ITCS6190-Task1-StreamingIngestion")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

# Schema for the incoming JSON
schema = StructType([
    StructField("trip_id", StringType(), True),
    StructField("driver_id", IntegerType(), True),
    StructField("distance_km", DoubleType(), True),
    StructField("fare_amount", DoubleType(), True),
    StructField("timestamp", StringType(), True),
])

# Read from socket (netcat-like stream)
raw = (
    spark.readStream
    .format("socket")
    .option("host", "localhost")
    .option("port", 9999)
    .load()
)

# Each line is a JSON object
parsed = raw.select(from_json(col("value"), schema).alias("json")).select("json.*")

# Print to console (required by Task 1)
console_query = (
    parsed.writeStream
    .format("console")
    .outputMode("append")
    .option("truncate", False)
    .start()
)

# Also persist parsed rows to CSV for grading convenience
csv_query = (
    parsed.writeStream
    .format("csv")
    .outputMode("append")
    .option("path", "outputs/task_1")
    .option("checkpointLocation", "outputs/task_1/_checkpoints")
    .option("escape", "\\")
    .start()
)

spark.streams.awaitAnyTermination()

/home/data/akhalegh/utils/spark-structured-streaming/data_generator.py
================================================================================
import socket
import json
import time
import random
from faker import Faker

fake = Faker()

# Generate a random ride event
def generate_ride_event():
    return {
        "trip_id": fake.uuid4(),
        "driver_id": random.randint(1, 100),
        "distance_km": round(random.uniform(1, 50), 2),
        "fare_amount": round(random.uniform(5, 150), 2),
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }

# Start streaming using socket
def start_streaming(host="localhost", port=9999):
    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    server_socket.bind((host, port))
    server_socket.listen(5)  # Increase backlog to allow multiple connections
    print(f"Streaming data to {host}:{port}...")

    while True:
        try:
            conn, addr = server_socket.accept()
            print(f"New client connected: {addr}")

            while True:
                try:
                    # Generate a ride event and send it to the connected client
                    ride_event = generate_ride_event()
                    conn.send((json.dumps(ride_event) + "\n").encode("utf-8"))
                    print("Sent:", ride_event)
                    time.sleep(1)
                except (BrokenPipeError, ConnectionResetError):
                    print(f"Client {addr} disconnected. Waiting for a new client.")
                    break  # Exit the inner loop and wait for a new client

        except Exception as e:
            print(f"Error accepting connection: {e}")

if __name__ == "__main__":
    start_streaming()


/home/data/akhalegh/utils/spark-structured-streaming/task3.py
================================================================================
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, to_timestamp, window, sum as _sum
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

spark = (
    SparkSession.builder
    .appName("ITCS6190-Task3-WindowedAnalytics")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

schema = StructType([
    StructField("trip_id", StringType(), True),
    StructField("driver_id", IntegerType(), True),
    StructField("distance_km", DoubleType(), True),
    StructField("fare_amount", DoubleType(), True),
    StructField("timestamp", StringType(), True),
])

raw = (
    spark.readStream
    .format("socket")
    .option("host", "localhost")
    .option("port", 9999)
    .load()
)

df = raw.select(from_json(col("value"), schema).alias("json")).select("json.*")

# Convert to event time and apply watermark (1 minute) to control state size
df = df.withColumn("event_time", to_timestamp(col("timestamp"))).withWatermark("event_time", "1 minute")

# 5-minute window, sliding by 1 minute
win_agg = (
    df.groupBy(window(col("event_time"), "5 minutes", "1 minute"))
      .agg(_sum("fare_amount").alias("sum_fare_amount"))
      .select(
          col("window.start").alias("window_start"),
          col("window.end").alias("window_end"),
          col("sum_fare_amount")
      )
)

def write_batch(batch_df, batch_id: int):
    (
        batch_df
        .orderBy("window_start")              # OK here: batch_df is static
        .coalesce(1)
        .write
        .mode("append")
        .option("header", True)
        .csv(f"outputs/task_3/batch_{batch_id:06d}")
    )

query = (
    win_agg.writeStream
    .outputMode("append")  # append is correct with window + watermark
    .foreachBatch(write_batch)
    .option("checkpointLocation", "outputs/task_3/_checkpoints")
    .start()
)

query.awaitTermination()

/home/data/akhalegh/utils/spark-structured-streaming/task2.py
================================================================================
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, sum as _sum, avg as _avg, to_timestamp
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType

# Spark session
spark = (
    SparkSession.builder
    .appName("ITCS6190-Task2-DriverAggregations")
    .getOrCreate()
)
spark.sparkContext.setLogLevel("WARN")

schema = StructType([
    StructField("trip_id", StringType(), True),
    StructField("driver_id", IntegerType(), True),
    StructField("distance_km", DoubleType(), True),
    StructField("fare_amount", DoubleType(), True),
    StructField("timestamp", StringType(), True),
])

raw = (
    spark.readStream
    .format("socket")
    .option("host", "localhost")
    .option("port", 9999)
    .load()
)

df = raw.select(from_json(col("value"), schema).alias("json")).select("json.*")

# Convert event time for watermarking (optional but good practice)
df = df.withColumn("event_time", to_timestamp(col("timestamp")))

agg = (
    df
    .groupBy("driver_id")
    .agg(
        _sum("fare_amount").alias("total_fare"),
        _avg("distance_km").alias("avg_distance"),
    )
)

# Use foreachBatch to write a single CSV with headers per micro-batch
def write_batch(batch_df, batch_id: int):
    (
        batch_df.coalesce(1)
        .write
        .mode("append")
        .option("header", True)
        .csv(f"outputs/task_2/batch_{batch_id:06d}")
    )

query = (
    agg.writeStream
    .outputMode("complete")  # complete mode needed for aggregations without watermark/window
    .foreachBatch(write_batch)
    .option("checkpointLocation", "outputs/task_2/_checkpoints")
    .start()
)

query.awaitTermination()

/home/data/akhalegh/utils/spark-structured-streaming/README.md
================================================================================
# Ride Sharing Analytics Using Spark Streaming and Spark SQL

## Overview
This repository implements a real-time analytics pipeline for a ride‑sharing platform using **Apache Spark Structured Streaming**. It ingests simulated ride events from a socket, parses JSON into structured columns, performs **driver‑level aggregations**, and computes **time‑windowed analytics**. Outputs are written to CSV for inspection and grading.

---

## Repository Structure

```
.
├── data_generator.py            # Streams JSON events over a TCP socket (localhost:9999)
├── task1.py                     # Task 1: Ingestion + parsing (prints to console + CSV)
├── task2.py                     # Task 2: Driver-level aggregations (SUM fare, AVG distance)
├── task3.py                     # Task 3: 5-min windowed sums with 1-min slide + watermark
├── outputs/
│   ├── task_1/                  # Raw parsed rows (CSV, many small part files)
│   ├── task_2/                  # Aggregates written per micro-batch (CSV per batch_NNNNNN/)
│   ├── task_3/                  # Windowed results per micro-batch (CSV per batch_NNNNNN/)
│   ├── task_1_samples/          # Selected sample CSVs for Task 1 (3 files)
│   ├── task_2_samples/          # Selected sample CSVs for Task 2 (3 files)
│   └── task_3_samples/          # Selected sample CSVs for Task 3 (3 files)
└── README.md                    # (This file)
```

---

## How to Run (Codespaces or Local)

1) **Install dependencies**
```bash
pip install pyspark faker
```

2) **Start the data generator** (Terminal A)
```bash
python data_generator.py
```
This opens `0.0.0.0:9999` and continually emits JSON events like:
```json
{"trip_id":"...","driver_id":54,"distance_km":25.25,"fare_amount":100.11,"timestamp":"2025-10-14 17:18:05"}
```

3) **Run the tasks** (each in its own terminal)

**Task 1**
```bash
python task1.py
```
- Reads from the socket with `spark.readStream.format("socket")`.
- Parses JSON into columns: `trip_id, driver_id, distance_km, fare_amount, timestamp`.
- Prints to console and writes CSVs to `outputs/task_1/`.

**Task 2**
```bash
python task2.py
```
- Reuses parsed fields, groups by `driver_id`.
- Computes:
  - `SUM(fare_amount)` → `total_fare`
  - `AVG(distance_km)` → `avg_distance`
- Writes one CSV per micro-batch under `outputs/task_2/batch_*/`.

**Task 3**
```bash
python task3.py
```
- Converts `timestamp` → `event_time` (TimestampType), applies `withWatermark("event_time","1 minute")`.
- 5‑minute **window** with **1‑minute slide**; aggregates `SUM(fare_amount)`.
- Sorts results **inside** `foreachBatch` (static DF) and writes to `outputs/task_3/batch_*/`.
- Let it run ~6–7 minutes (≈35+ micro-batches) to produce non‑empty window results.

---

## Requirements → Implementation Mapping

| Requirement | Implementation |
|---|---|
| Ingest from socket (localhost:9999) | `spark.readStream.format("socket").option("host","localhost").option("port",9999)` |
| Parse JSON into columns | `from_json(col("value"), schema).alias("json").select("json.*")` |
| Print parsed data to console | `writeStream.format("console").outputMode("append")` (Task 1) |
| Driver-level real-time aggregations | `groupBy("driver_id")` + `sum(fare_amount)`, `avg(distance_km)` (Task 2) |
| Write aggregations to CSV | `foreachBatch` writing `outputs/task_2/batch_*/` |
| Time-windowed analytics | `withWatermark("event_time","1 minute")`, `window("5 minutes","1 minute")` (Task 3) |
| Write windowed results to CSV | `foreachBatch` to `outputs/task_3/batch_*/` |

---

## Sample Results (from actual run)

### Task 1 — Parsed Rows (3 examples)
```
d34e5277-8fd6-4067-8eec-5d63cd06535f,23,39.99,62.85,2025-10-14 17:20:57
55a0a604-202b-4ca8-9520-b938833fa867,49,25.43,8.72,2025-10-14 17:20:58
f669a3b1-834c-40cd-97ac-bcf82333ac8c,19,44.6,118.66,2025-10-14 17:20:56
```

### Task 2 — Driver Aggregations (3 snapshots)
_Example excerpt (columns: `driver_id,total_fare,avg_distance`):_
```
65,77.11,26.55
78,164.29,23.65
81,215.8,27.56
...
```
```
65,16.48,4.31
78,164.29,23.65
81,192.07,21.83
...
```
```
65,77.11,26.55
78,164.29,23.65
81,192.07,21.83
...
```

> _Note:_ Long floats are normal from Spark. If desired, round in code with `round(sum(...), 2)` and `round(avg(...), 2)`.

### Task 3 — 5‑Minute Windows (1‑minute slide; watermark 1m)
```
window_start,window_end,sum_fare_amount
2025-10-14T17:22:00.000Z,2025-10-14T17:27:00.000Z,1626.91
2025-10-14T17:23:00.000Z,2025-10-14T17:28:00.000Z,5892.83
2025-10-14T17:29:00.000Z,2025-10-14T17:34:00.000Z,23518.80
```

---

## Troubleshooting

- **Empty Task 3 files:** Many micro-batches won’t contain a *completed* 5‑min window yet. Let Task 3 run ~6–7 minutes. You can also skip writing empty batches in `foreachBatch` by checking `batch_df.rdd.isEmpty()`.  
- **“Sorting not supported” error:** Sort streaming results **inside `foreachBatch`** (the micro‑batch is static) or drop the sort.  
- **Port forwarding in Codespaces:** Ensure port **9999** is forwarded; the generator prints a “New client connected” message when Task 1/2/3 attaches.  
- **Spark UI port in use:** Spark will auto-increment the UI port (4040 → 4041 → 4042). This is informational only.



/home/data/akhalegh/utils/spark-structured-streaming/outputs/task_3_samples/part-00000-ac754502-3e6e-41cb-bc0a-fec573e1f6c9-c000.csv
================================================================================
window_start,window_end,sum_fare_amount
2025-10-14T17:22:00.000Z,2025-10-14T17:27:00.000Z,1626.91


/home/data/akhalegh/utils/spark-structured-streaming/outputs/task_3_samples/part-00000-41e3f20d-35a3-4351-b1eb-29cedf8cb6e1-c000.csv
================================================================================
window_start,window_end,sum_fare_amount
2025-10-14T17:29:00.000Z,2025-10-14T17:34:00.000Z,23518.8


/home/data/akhalegh/utils/spark-structured-streaming/outputs/task_3_samples/part-00000-cb047fb3-3acd-45f7-a900-cc4a4605ce62-c000.csv
================================================================================
window_start,window_end,sum_fare_amount
2025-10-14T17:23:00.000Z,2025-10-14T17:28:00.000Z,5892.83


/home/data/akhalegh/utils/spark-structured-streaming/outputs/task_2_samples/part-00000-33b3c6ca-6402-43c7-a6f9-3230cbb92e74-c000.csv
================================================================================
driver_id,total_fare,avg_distance
65,77.11,26.55
78,164.29000000000002,23.65
81,192.07000000000002,21.830000000000002
28,114.3,14.69
76,346.47,18.7925
44,97.41,29.185
12,120.33,47.11
91,141.04,1.68
22,142.31,21.81
93,18.82,1.53
47,107.2,30.78
52,81.11,41.31
13,71.28,41.64
6,141.51,26.213333333333335
86,129.1,16.53
94,30.6,33.51
57,87.76,6.6049999999999995
54,522.72,24.144
96,88.53,5.28
48,93.84,22.6
19,33.0,23.41
64,99.97,6.22
41,49.5,15.33
15,83.38000000000001,18.84
43,101.12,34.72
37,142.07999999999998,25.98
61,124.75999999999999,19.34
9,102.1,18.155
17,169.32,36.35666666666666
35,143.0,35.99
59,378.98,22.505000000000003
55,148.79,23.13
39,173.20000000000002,10.09
23,206.15999999999997,33.245
49,122.91,29.75
87,85.55,40.7
51,210.35,31.13
97,112.07,38.38
10,57.11,24.86
45,50.68,10.46
38,25.81,6.32
82,84.29,41.89
80,31.71,34.89
24,166.8,35.51
70,132.94,34.0
62,95.41,49.97
95,57.55,49.98
29,16.36,22.64
21,120.98,36.0
98,140.51,41.21
90,67.92,40.78
32,109.97,27.94
75,236.5,33.64000000000001
56,35.32,9.7
58,253.35000000000002,40.59
33,116.91,17.8
68,155.08,37.25
71,233.46999999999997,29.75
14,257.48,18.6825
42,235.32999999999998,19.41
79,292.13,44.74333333333333
2,169.14,17.155
30,215.56,21.39
66,396.07000000000005,25.403333333333336
67,86.4,25.54
46,26.01,23.71
36,80.57,29.33
89,178.82999999999998,29.604999999999997


/home/data/akhalegh/utils/spark-structured-streaming/outputs/task_2_samples/part-00000-27fe34ce-6212-4f58-b76e-e4ada7ecba20-c000.csv
================================================================================
driver_id,total_fare,avg_distance
65,16.48,4.31
78,164.29000000000002,23.65
81,192.07000000000002,21.830000000000002
28,114.3,14.69
76,218.85,15.32
44,97.41,29.185
12,120.33,47.11
91,141.04,1.68
22,142.31,21.81
52,81.11,41.31
13,71.28,41.64
6,132.47,17.86
86,129.1,16.53
94,30.6,33.51
57,87.76,6.6049999999999995
54,522.72,24.144
96,88.53,5.28
48,93.84,22.6
19,33.0,23.41
64,99.97,6.22
41,49.5,15.33
15,83.38000000000001,18.84
43,101.12,34.72
37,108.46,43.2
61,124.75999999999999,19.34
9,102.1,18.155
17,160.35999999999999,29.875
59,378.98,22.505000000000003
55,36.96,35.32
39,142.68,8.21
23,206.15999999999997,33.245
49,122.91,29.75
87,85.55,40.7
51,210.35,31.13
97,112.07,38.38
10,57.11,24.86
45,50.68,10.46
38,25.81,6.32
82,84.29,41.89
80,31.71,34.89
24,166.8,35.51
70,132.94,34.0
62,95.41,49.97
95,57.55,49.98
29,16.36,22.64
21,120.98,36.0
98,140.51,41.21
90,67.92,40.78
32,109.97,27.94
75,134.05,34.165000000000006
56,35.32,9.7
58,253.35000000000002,40.59
33,116.91,17.8
68,7.99,47.41
71,233.46999999999997,29.75
14,157.49,11.886666666666668
42,131.23,37.56
79,292.13,44.74333333333333
2,169.14,17.155
30,215.56,21.39
66,396.07000000000005,25.403333333333336
67,86.4,25.54
46,26.01,23.71
36,80.57,29.33
89,164.54,33.346666666666664


/home/data/akhalegh/utils/spark-structured-streaming/outputs/task_2_samples/part-00000-0a853991-4261-4cc5-a38f-0842b594b58c-c000.csv
================================================================================
driver_id,total_fare,avg_distance
65,77.11,26.55
78,164.29000000000002,23.65
81,215.8,27.560000000000002
28,114.3,14.69
76,346.47,18.7925
27,127.95,3.8
44,97.41,29.185
12,120.33,47.11
91,141.04,1.68
22,149.22,29.85
93,18.82,1.53
47,238.51,27.28
1,105.16,38.99
52,81.11,41.31
13,71.28,41.64
6,141.51,26.213333333333335
86,129.1,16.53
20,29.1,37.03
94,30.6,33.51
57,87.76,6.6049999999999995
54,522.72,24.144
96,88.53,5.28
48,93.84,22.6
19,33.0,23.41
92,45.11,21.9
64,99.97,6.22
41,49.5,15.33
15,83.38000000000001,18.84
43,101.12,34.72
37,142.07999999999998,25.98
61,124.75999999999999,19.34
9,102.1,18.155
17,169.32,36.35666666666666
35,143.0,35.99
59,378.98,22.505000000000003
55,148.79,23.13
39,173.20000000000002,10.09
23,206.15999999999997,33.245
49,122.91,29.75
84,109.08,15.14
87,85.55,40.7
51,210.35,31.13
97,112.07,38.38
10,57.11,24.86
45,50.68,10.46
38,25.81,6.32
82,84.29,41.89
80,95.66,24.655
24,166.8,35.51
70,132.94,34.0
62,95.41,49.97
95,112.63,27.189999999999998
29,16.36,22.64
21,207.5,22.84
98,140.51,41.21
90,67.92,40.78
32,109.97,27.94
75,236.5,33.64000000000001
56,35.32,9.7
58,253.35000000000002,40.59
33,116.91,17.8
68,155.08,37.25
71,233.46999999999997,29.75
14,257.48,18.6825
42,235.32999999999998,19.41
79,292.13,44.74333333333333
2,169.14,17.155
30,215.56,21.39
66,396.07000000000005,25.403333333333336
67,86.4,25.54
46,94.87,21.755000000000003
36,80.57,29.33
89,300.83,24.886


/home/data/akhalegh/utils/spark-structured-streaming/outputs/task_1_samples/part-00000-26dbff93-1398-4ceb-8476-42791e981115-c000.csv
================================================================================
d34e5277-8fd6-4067-8eec-5d63cd06535f,23,39.99,62.85,2025-10-14 17:20:57


/home/data/akhalegh/utils/spark-structured-streaming/outputs/task_1_samples/part-00000-c37c7140-3a75-4387-90e8-6c4e43ffa00c-c000.csv
================================================================================
55a0a604-202b-4ca8-9520-b938833fa867,49,25.43,8.72,2025-10-14 17:20:58


/home/data/akhalegh/utils/spark-structured-streaming/outputs/task_1_samples/part-00000-db00ef55-2a04-4b10-bb9f-08313ba7aee6-c000.csv
================================================================================
f669a3b1-834c-40cd-97ac-bcf82333ac8c,19,44.6,118.66,2025-10-14 17:20:56


